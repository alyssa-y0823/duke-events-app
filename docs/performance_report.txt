Performance Benchmark Report

Goal: Measure inference time, throughput, and computational efficiency of the embedding + ranking system.

Metrics Measured:
- Throughput (events processed per second)
- Latency per event (avg embedding time)
- End-to-end inference time (embed query + rank against 163 events)

Results:
- Model load time: 1.88s
- Embedding throughput: 25.23 events/sec (163 events in ~6.5s)
- Average event latency: 39.63ms
- Query latency: 1.76s
- Note: High outlier; expected ~40–100ms based on event latency
- Scoring latency: 2.04ms (vector operations are negligible)

Analysis:
- System can process ~1,500 events/min on a single CPU core — enough for Duke Events needs.
- Main bottleneck: sentence-transformers embedding step.
- Current method encodes one text at a time.
- Batch encoding (model.encode(texts)) would likely give 10–50× throughput improvement.
- Query embedding determines perceived latency.
- Scoring 160+ events is effectively instant (<3ms).
- Even with the 1.7s outlier, overall latency is <2s; expected normal latency ~50ms enables real-time interaction.

Conclusion:
- System is efficient enough for production on modest hardware.
- Precomputing event embeddings is sufficient.
- Real-time re-ranking is fully feasible.